@article{Sharma_Agarwal_Suresh_Nakov_Akhtar_Chakraborty_2023, title={What Do You MEME? Generating Explanations for Visual Semantic Role Labelling in Memes}, volume={37}, url={https://ojs.aaai.org/index.php/AAAI/article/view/26166}, DOI={10.1609/aaai.v37i8.26166}, abstractNote={Memes are powerful means for effective communication on social media. Their effortless amalgamation of viral visuals and compelling messages can have far-reaching implications with proper marketing. Previous research on memes has primarily focused on characterizing their affective spectrum and detecting whether the memeâ€™s message insinuates any intended harm, such as hate, offense, racism, etc. However, memes often use abstraction, which can be elusive. Here, we introduce a novel task - EXCLAIM, generating explanations for visual semantic role labeling in memes. To this end, we curate ExHVV, a novel dataset that offers natural language explanations of connotative roles for three types of entities - heroes, villains, and victims, encompassing 4,680 entities present in 3K memes. We also benchmark ExHVV with several strong unimodal and multimodal baselines. Moreover, we posit LUMEN, a novel multimodal, multi-task learning framework that endeavors to address EXCLAIM optimally by jointly learning to predict the correct semantic roles and correspondingly to generate suitable natural language explanations. LUMEN distinctly outperforms the best baseline across 18 standard natural language generation evaluation metrics. Our systematic evaluation and analyses demonstrate that characteristic multimodal cues required for adjudicating semantic roles are also helpful for generating suitable explanations.}, number={8}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Sharma, Shivam and Agarwal, Siddhant and Suresh, Tharun and Nakov, Preslav and Akhtar, Md. Shad and Chakraborty, Tanmoy}, year={2023}, month={Jun.}, pages={9763-9771} }