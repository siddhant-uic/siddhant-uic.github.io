@inproceedings{agarwal-etal-2024-mememqa,
    title = "{M}eme{MQA}: Multimodal Question Answering for Memes via Rationale-Based Inferencing",
    author = "Agarwal, Siddhant  and
      Sharma, Shivam  and
      Nakov, Preslav  and
      Chakraborty, Tanmoy",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand and virtual meeting",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.300",
    pages = "5042--5078",
    abstract = "Memes have evolved as a prevalent medium for diverse communication, ranging from humour to propaganda. With the rising popularity of image-focused content, there is a growing need to explore its potential harm from different aspects. Previous studies have analyzed memes in closed settings - detecting harm, applying semantic labels, and offering natural language explanations. To extend this research, we introduce MemeMQA, a multimodal question-answering framework aiming to solicit accurate responses to structured questions while providing coherent explanations. We curate MemeMQACorpus, a new dataset featuring 1,880 questions related to 1,122 memes with corresponding answer-explanation pairs. We further propose ARSENAL, a novel two-stage multimodal framework that leverages the reasoning capabilities of LLMs to address MemeMQA. We benchmark MemeMQA using competitive baselines and demonstrate its superiority - {\textasciitilde}18{\%} enhanced answer prediction accuracy and distinct text generation lead across various metrics measuring lexical and semantic alignment over the best baseline. We analyze ARSENAL{'}s robustness through diversification of question-set, confounder-based evaluation regarding MemeMQA{'}s generalizability, and modality-specific assessment, enhancing our understanding of meme interpretation in the multimodal communication landscape.",
}
@inproceedings{Sharma_Agarwal_Suresh_Nakov_Akhtar_Chakraborty_2023, title={What Do You MEME? Generating Explanations for Visual Semantic Role Labelling in Memes}, volume={37}, url={https://ojs.aaai.org/index.php/AAAI/article/view/26166}, DOI={10.1609/aaai.v37i8.26166}, abstractNote={Memes are powerful means for effective communication on social media. Their effortless amalgamation of viral visuals and compelling messages can have far-reaching implications with proper marketing. Previous research on memes has primarily focused on characterizing their affective spectrum and detecting whether the memeâ€™s message insinuates any intended harm, such as hate, offense, racism, etc. However, memes often use abstraction, which can be elusive. Here, we introduce a novel task - EXCLAIM, generating explanations for visual semantic role labeling in memes. To this end, we curate ExHVV, a novel dataset that offers natural language explanations of connotative roles for three types of entities - heroes, villains, and victims, encompassing 4,680 entities present in 3K memes. We also benchmark ExHVV with several strong unimodal and multimodal baselines. Moreover, we posit LUMEN, a novel multimodal, multi-task learning framework that endeavors to address EXCLAIM optimally by jointly learning to predict the correct semantic roles and correspondingly to generate suitable natural language explanations. LUMEN distinctly outperforms the best baseline across 18 standard natural language generation evaluation metrics. Our systematic evaluation and analyses demonstrate that characteristic multimodal cues required for adjudicating semantic roles are also helpful for generating suitable explanations.}, number={8}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Sharma, Shivam and Agarwal, Siddhant and Suresh, Tharun and Nakov, Preslav and Akhtar, Md. Shad and Chakraborty, Tanmoy}, year={2023}, month={Jun.}, pages={9763-9771} }
@article{agarwal2023meme,
  title={Meme analysis},
  author={Agarwal, Siddhant and Akhtar, Md Shad},
  year={2023},
  publisher={IIIT-Delhi},
url = "http://repository.iiitd.edu.in/xmlui/handle/123456789/1471"
}
