@inproceedings{agarwal-etal-2024-mememqa,
 abstract = {Memes have evolved as a prevalent medium for diverse communication, ranging from humour to propaganda. With the rising popularity of image-focused content, there is a growing need to explore its potential harm from different aspects. Previous studies have analyzed memes in closed settings - detecting harm, applying semantic labels, and offering natural language explanations. To extend this research, we introduce MemeMQA, a multimodal question-answering framework aiming to solicit accurate responses to structured questions while providing coherent explanations. We curate MemeMQACorpus, a new dataset featuring 1,880 questions related to 1,122 memes with corresponding answer-explanation pairs. We further propose ARSENAL, a novel two-stage multimodal framework that leverages the reasoning capabilities of LLMs to address MemeMQA. We benchmark MemeMQA using competitive baselines and demonstrate its superiority - ~18% enhanced answer prediction accuracy and distinct text generation lead across various metrics measuring lexical and semantic alignment over the best baseline. We analyze ARSENAL′s robustness through diversification of question-set, confounder-based evaluation regarding MemeMQA′s generalizability, and modality-specific assessment, enhancing our understanding of meme interpretation in the multimodal communication landscape.},
 address = {Bangkok, Thailand and virtual meeting},
 author = {Agarwal, Siddhant  and
Sharma, Shivam  and
Nakov, Preslav  and
Chakraborty, Tanmoy},
 booktitle = {Findings of the Association for Computational Linguistics ACL 2024},
 editor = {Ku, Lun-Wei  and
Martins, Andre  and
Srikumar, Vivek},
 month = {August},
 pages = {5042--5078},
 publisher = {Association for Computational Linguistics},
 title = {MemeMQA: Multimodal Question Answering for Memes via Rationale-Based Inferencing},
 url = {https://aclanthology.org/2024.findings-acl.300},
 year = {2024}
}
